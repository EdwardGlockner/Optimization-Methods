# Optimization algorithms 


## About the Project
This is a project on different optimization algorithms. Currently this program has implement standard Gradient Descent and Stochastic Gradient Descent. The program currently only works for 1D functions, but soon 2D function will be added aswell.

<!-- Table of Contents -->
## Table of Contents
  - [Introduction](#introduction)
  - [Run Locally](#run-locally)

  - [Contact](#contact)
  - [Links](#links)

---
## Introduction
A GUI using PyQT5 is displayed where the user can input a function, learning rate and starting guess. The program then parses the equation into a derivative which is then used to try to find the minimum. The function, the calculated values from the optimization algorithm and the derivative is then interactively displayed in the GUI.

---
<!-- Installation -->
## Run Locally
Clone the repository

```bash
  git clone https://github.com/EdwardGlockner/Optimization-Methods.git
```

Go to the project directory

```bash
  cd my-project
```

Install the necessary libraries by running the `install.py`. The python version for which the libraries will be installed for can be modified through the -v flag, which is set to none by default.

```bash
  python install.py -v <python-version>
```

Go to `src/` and run `main.py`

---
<!-- Contact -->
## Contact
Edward Gl√∂ckner - [@linkedin](https://www.linkedin.com/in/edwardglockner/) - edward.glockner5@gmail.com

Project Link: [https://github.com/EdwardGlockner/Optimization-Methods](https://github.com/EdwardGlockner/Optimization-Methods)

---
<!-- Links -->
## Links

Here are some helpful links:

